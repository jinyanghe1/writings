# Transformer

[Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=f3c6b23f75d311364ca35ca0efb06c7f)

感谢李沐23333

# 介绍

摘要中作者提出了一个简单的网络架构（Transformer），这个架构纯粹基于注意力机制，而不依赖于之前的卷积/循环神经网络架构（曾经的主流架构）。他们的模型在En-De机器翻译中拔得头筹，并且训练开销比较低，也具有良好的可拓展性。

# 结论

在总结阶段中，他们再次强调Transformer在时间复杂度上的优越性，同时介绍说Transformer的架构在将来（现在已经被实现啦）可能可以被用于处理多模态问题，比如语音、视频等等。

# 导言

RNN模型的简要原理是：对于新的输入，输出同时由之前的隐藏状态$h_t$和当前时刻的输入$x$同时决定。这可以保证网络能够学习到一些时序上的信息。

这个架构有两个“硬伤”：

1. 难以并行。$o_t$的输出必须依赖于$t$时刻得到的隐藏状态。
2. 难以保留“久远”的上下文信息。

作为改进，作者团队提出基于注意力机制的网络架构，这种架构能够在输入和输出之间建立全局的关系。

# 相关工作

之前有很多相关工作希望通过卷积神经网络来代替循环神经网络，这样就可以提高并行度。这种思路的问题在于随着输入和输出的“距离”拉远，第$t$步的输入越来越难以包含上下文信息。

另一方面，卷积可以通过不同的通道来学习不同的模式，基于这一点，作者团队提出multi-head attention和self-attention这一算法。

这个工作的创新之处在于它是第一个完全依赖于自注意力来进行转录的模型。

# 模型

![在这里可以看到，输入并不是“直接转换”为输出，而是经过n个带有残差连接的注意力层+前馈神经网络，再进入输出的注意力层。](Transformer%20390021fbbe9c4423a0055109f3400442/Untitled.png)

在这里可以看到，输入并不是“直接转换”为输出，而是经过n个带有残差连接的注意力层+前馈神经网络，再进入输出的注意力层。

## 自回归

编码器将输入符号$(x_1,\dots,x_n)$转换成一个序列$\mathbf z=(z_1,\dots,z_n)$，解码器通过$\mathbf z$生成输出序列$(y_1,\dots,y_m)$，在生成输出序列的时候模型采用自回归机制，也就是说之前的输出会同时作为输入影响下一步输出。

## 编码器&解码器

- 编码器模块采用了六个相同的层。每个层有两个子层：
    1. 多头注意力机制
    2. 全连接前馈网络（其实就是mlp）
    
    每个层的输出维度都是$d_{model}=512$。每层的输出都采用了LayerNorm进行处理。这是因为在transformer架构中输入的数据是一个三维的tensor。下图可以看到这种方法其实是对不同的通道做了归一化。
    
    ![因为seq的长度不同，所以batch norm并不好用。](Transformer%20390021fbbe9c4423a0055109f3400442/Untitled%201.png)
    
    因为seq的长度不同，所以batch norm并不好用。
    
- 解码器模块也使用六个相同的层。解码器还加入了一个带掩码的多头注意力层。同时之前的输出被做了“偏移”处理，这是为了保证在预测$t$时刻的输出时不会读到$t$时刻之后的输出。（这里可能会愣一下，$t$时刻之后的输出不是还没出来吗？怎么会被读到呢？稍微思考一下，反应过来整个输入→输出的过程是并行化的，并且层间的运算要进行多次，在进行第$i$层的decoder的运算时当然已经获得了$i-1$层的全部信息）。

## attention

![https://miro.medium.com/v2/resize:fit:1400/1*Uya6_ec79IIXOkvPbGOQ-g.png](https://miro.medium.com/v2/resize:fit:1400/1*Uya6_ec79IIXOkvPbGOQ-g.png)

![https://theaisummer.com/static/e9145585ddeed479c482761fe069518d/ea64c/attention.png](https://theaisummer.com/static/e9145585ddeed479c482761fe069518d/ea64c/attention.png)

注意力函数可以用上面的图来形象表示。有

$$
\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

这也是这个算法高并行的秘诀之一。

## mask

为了保证$t$时刻的输出只看$k_1,\dots,k_{t-1}$的内容，在输出的时候加入一个mask层对$t$时刻后的位置乘上一个特别大的负数，这样的话softmax操作时候$t$时刻之后的内容带有的权重就会变得非常小。可以看到在上面网络结构图中，输出端使用的是带掩码的多头注意力。

## 多头注意力

![https://d2l.ai/_images/multi-head-attention.svg](https://d2l.ai/_images/multi-head-attention.svg)

query $\mathbf q\in \R ^{d_q}$，key $\mathbf k \in \R ^{d_k}$, value $\mathbf v\in\R ^{d_v}$。

头$i$的可学习参数有$W_i^{(q)}\in \R^{p_q\times d_q},W_i^{(k)}\in \R ^{p_k\times d_k}, W_i^{(v)}\in \R ^{\_v\times d_v}$。

输出端（全连接层）的可学习参数是$W_0\in \R ^{p_o\times hp_v}$。

## 位置编码

注意力机制本身希望将位置编码放到输入中，而自注意力本身不记录位置信息。

假设长度为$n$的序列$\mathbf X\in \R ^{n\times d}$，那么，使用位置编码矩阵$\mathbf P\in \R^{n\times d}$来输出$\mathbf {X+P}$作为自编码输入。$\mathbf P$的元素可以这样计算：

$$
p_{i,2j}=\sin(i/10000^{2j/d}),\\ p_{i,2j+1}=\cos(i/10000^{2j/d})
$$

这个编码有一个好处，我们来看一下位置编码递推式的矩阵表示：

$$
w_j:=1/10000^{2j/d}, \\\begin{bmatrix}\cos(\delta \omega_j)& \sin(\delta\omega_j) \\ -\sin(\delta \omega_j)&\cos (\delta \omega_j)\end{bmatrix}\begin{bmatrix} p_{i,2j}\\ p_{i,2j+1}\end{bmatrix}=\begin{bmatrix}p_{i+\delta,2j}\\ p_{i+\delta, 2j+1}\end{bmatrix}
$$

# 实验

# 结论