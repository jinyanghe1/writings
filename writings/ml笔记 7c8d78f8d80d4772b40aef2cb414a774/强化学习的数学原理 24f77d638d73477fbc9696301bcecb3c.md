# 强化学习的数学原理

# 基本概念

- 强化学习的范式：智能体做出决策`action`，改变`state` ，环境给予相应的反馈，即`reward` 。根据反馈智能体进行迭代更新。

![Untitled](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2024f77d638d73477fbc9696301bcecb3c/Untitled.png)

- 下面介绍强化学习的一些常用概念。我们想象一个grid world，智能体的目标是从起点走到目标位置。

![Untitled](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2024f77d638d73477fbc9696301bcecb3c/Untitled%201.png)

- `state` ：智能体相对环境的状态。比如，grid world中小球的位置。不同的`state` 构成一个状态空间，即`state space` ，用$S=\{s_i\}$表示。
- `action` ：智能体在某个特定的`state` 中可以采取的操作。比如在一个格子中，小球可以选择上下左右移动。不同的`state` 对应不同的`action space` 。记作$A(s_i)$。
- `state transition` ：智能体采取行动，导致从一个`state` 转变为另一个`state` 。记作$s_1\xrightarrow{a_2}s_2$。
- `forbidden area` ：比如，小球不能走进障碍物里。我们可以在建模的时候考虑两种情况：
    - `forbidden area` 可以进入，但是会受到惩罚
    - `forbidden area` 不可以进入，这时状态空间减少
- `state transition probability` ：用概率来描述状态转移。比如在grid world中，向右走一定会使小球从左边的格子走到右边的格子，这时状态转移是确定（deterministic）的。但是有的游戏的状态转移是随机（stochastic）的，比如涉及到掷骰子的游戏。
- `policy` ：告诉智能体在一个特定的`state` 应该采用何种`action` 。在grid world中，决定小球怎么走的规则（算法）就是`policy` 。用$\pi$来表示。策略可以是确定的，也可以是不确定的。一个不确定的策略表示给定一个`state` ，存在多种可能的`action` 。
- `reward` ：智能体采取一个行动后会得到一个`reward` 。`reward` 是一个实数，有
    
    $$
    \begin{cases}\text{reward}>0&\text{鼓励}\\ \text{reward}<0&\text{不鼓励}\end{cases}
    $$
    
    在grid world的例子里，可能当小球走出边界时，`reward` 为负，当小球走到目标格子时，`reward` 为正，不然`reward` 为0。
    
- `trajectory` ：一个`state-action-reward` 构成的链条。智能体采取一连串`action` ，使得`state` 不断改变，然后对于每个`action` 得到对应的`reward` 。
- `return` ：在一个`trajectory` 中智能体得到的`reward` 的总和。
- `discounted return` ：一个`trajectory` 可以是无限长的，这样有可能会出现`return` 不收敛（变成无限大）的情况。为了更好评估不同`trajectory` 的优劣，引入`discount rate` $\gamma \in(0,1]$，对于第$n$个`reward` ，有$\text{discounted reward}=\text{reward}*\gamma^n$。
- `episode` ：有的任务可能没有终止状态，这样智能体与环境的交互不会终止，我们可以用统一的数学方法来统一有/没有终止状态的任务：智能体达到终止状态后可以离开这个状态（这个操作对应一个`reward`），也可以选择待在这个状态（对应有终止状态的情况）而一直获得固定的`reward`。重新回到这个状态的时候可以得到正`reward` 。
- `markov decision process` ：马尔科夫决策过程有一个重要的性质，即其新的操作是与历史无关的。

# 贝尔曼公式

## `return` 很重要！

return可以帮助我们评估不同的策略。

我们希望有一种能够定量计算`return` 的方法。

1. 第一种方法比较简单，就是按照定义，每一个`action` 对应的`reward` 加起来，就得到`return` 。
2. 对于下图，有
    
    ![Untitled](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2024f77d638d73477fbc9696301bcecb3c/Untitled%202.png)
    
    $$
    v_1=r_1+\gamma r_2+\gamma^2r_3+\dots \\ v_2 = r_2+\gamma r_2+\gamma^2 r_4+\dots \\ \dots
    $$
    
    可以稍作变化得到下面的递推公式：
    
    $$
    v_1=r_1+\gamma v_2 \\ v_2=r_2+\gamma v_3\\ v_3 = r_3+\gamma v_4 \\v_4=r_4+\gamma v_1
    $$
    
    可以重写为矩阵形式：
    
    $$
    \mathbf v = \mathbf r+\gamma \mathbf P\mathbf v
    $$
    
    其实这就是所谓的贝尔曼公式（Bellman equation）！但是目前的公式只能用于解决最简单的、确定的问题。
    

## state value

对多个`trajectory` 求平均值，得到这个状态的`state value` 。这是因为一个`state` 可能对应多种`trajectory` ，而当每个`state` 和`trajectory` 唯一确定的时候，`trajectory` 和`state value` 就是一样的。

## 贝尔曼公式

下面推导贝尔曼公式。

### 推导

考虑随机的一条`trajectory` ，可以求解`return` $G_t$：

$$
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots\\ = R_{t+1}+\gamma G_{t+1}
$$

进而有

$$
v_\pi(s)=\mathbb E[G_t|S_t=s]\\ = \mathbb E[R_{t+1}+\gamma G_{t+1}|S_t=s]\\ = \mathbb E[R_{t+1}|S_t=s]+\gamma \mathbb E[G_{t+1}|S_t=s]
$$

对$\mathbb E[R_{t+1}|S_t=s]$可以求解：

$$
\mathbb E[R_{t+1}|S_t=s] = \sum_a \pi(a|s)\mathbb E[R_{t+1}|S_t=s,A_t=a] \\ = \sum_a\pi(a|s)\sum_rp(r|s,a)r
$$

对于上面的第二项，有：

$$
\mathbb E[G_{t+1}|S_t=s]=\sum_{s'}\mathbb E[G_{t+1}|S_t=s,S_{t+1}=s']p(s'|s)\\ =\sum_{s'}v_\pi (s')p(s'|s)\\ =\sum_{s'}v_\pi(s')\sum_ap(s'|s,a)\pi(a|s)
$$

得到Bellman equation:

$$
v_\pi(s)=\sum_a\pi(a|s)[\sum_rp(r|s,a)r+\gamma\sum_{s'}p(s'|s,a)v_\pi(s')],\forall s\in S
$$

同样，对每一个状态，都有这样一个公式来计算其`state value` ，要使用像前面的bootstrapping的方法来进行计算。

### vetorization

先对上面的式子进行重写：

$$
v_\pi(s_i)=r_\pi(s_i)+\gamma \sum_{s_j}p_\pi(s_j|s_i)v_\pi(s_j)
$$

可以比较容易写成向量形式：

$$
v_\pi = r_\pi +\gamma P_\pi v_\pi\\ v_\pi = [v_\pi(s_1),v_\pi(s_2),\dots,v_\pi(s_n)]^T\\ r_\pi = [r_\pi(s_1),\dots,r_\pi(s_n)]^T\\ P_\pi \in \R^{n\times n},[P_\pi]_{ij}=p_\pi(s_j|s_i)
$$

### 求解

上面的矩阵表达式可以很好被求解。

$$
v_\pi = (I-\gamma P_\pi)^{-1}r_\pi
$$

在具体的求解时，当状态空间很大的时候，矩阵求逆的难度比较大。可以采取迭代求解的方法：

$$
v_{k+1}=r_\pi +\gamma P_\pi v_k
$$

## action value

我们希望知道对于一个特定的状态，一个`action` 的价值。

定义：

$$
q_\pi(s,a)=\mathbb E[G_t|S_t=s,A_t=a]
$$

通过贝尔曼公式，可以得到

$$
q_\pi(s,a)=\sum_rp(r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_\pi(s')
$$

# 贝尔曼最优公式

# 值迭代、策略迭代

# 蒙特卡洛方法

# 随机近似与随机梯度下降

# 时序差分方法

# 值函数近似

# 策略梯度方法

# actor-critic方法