# 图深度学习笔记

# 图嵌入表示学习

[Graph Embeddings](https://www.youtube.com/watch?v=oQPCxwmBiWo)

本小节主要信息来源

- 图表示学习的一个重要动机：需要找到一个合适的方法把图中顶点、边之间的关系用矩阵来描述，因为用矩阵描述的问题可以用传统的机器学习算法来解决。
- 需要找到一个合适的方法将节点编码（encode）为一个向量，同时节约内存。考虑Facebook的用户关系网，用邻接矩阵描述这个关系的内存开销显然过于庞大了。

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled.png)

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%201.png)

## deepwalk方法

[DeepWalk【图神经网络论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1o94y197vf/?spm_id_from=333.337.search-card.all.click&vd_source=f3c6b23f75d311364ca35ca0efb06c7f)

本小节内容的主要知识来源

- deepwalk是对节点进行随机游走来抽取特征的方法。deepwalk可以将一张图转换为连续稠密的矩阵。每个向量都隐式包含了图中的社群、连接、结构信息。

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%202.png)

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%203.png)

- Deepwalk直接套用node to vec方法。

## node to vec方法

[Papers with Code - node2vec Explained](https://paperswithcode.com/method/node2vec)

[Graph Embeddings (node2vec) explained - How nodes get mapped to vectors](https://www.youtube.com/watch?v=pS_POUVFXvk)

[Node2Vec【图神经网络论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1BS4y1E7tf/?spm_id_from=333.788.recommend_more_video.3&vd_source=f3c6b23f75d311364ca35ca0efb06c7f)

本小节内容的主要知识来源

- 采用有偏的随机游走。从上一个节点`a` 走到`b` 后，有一定的几率返回，一定的几率$p$走到与`a` 距离为$1$的节点（徘徊），一定的几率$q$走到与`a` 距离为$2$的节点（远行）。根据$p,q$的值的不同，随机游走会偏向于DFS和BFS，从而获得图的不同信息。
- 给定节点`u,v` ，希望建立映射`f(u)` （得到一个向量）。$f(u)\cdot f(v)$刻画的是在“看到”`u` 时同时“看到”`v` 的概率。
    - 进而，希望优化目标函数：$\max P(N_S(u)|u)$，即看到`u` 时同时看到其他近邻点的概率。
    - 独立性假设：$\max \Pi_{v_i\in {N_s}(u)}P(v_i|u)$.
    - 对图中的所有点进行评估：$\sum_{u\in V}\log P(N_s(u)|u)$.
    - 运用SGD进行回归任务

# 图卷积神经网络

[图卷积神经网络GCN_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Hs4y157Ls/?spm_id_from=333.788.recommend_more_video.0&vd_source=f3c6b23f75d311364ca35ca0efb06c7f)

[Graph Convolutional Networks (GCNs) made simple](https://www.youtube.com/watch?v=2KRAOZIULzw)

## 计算图

- 通过局部邻域构建计算图。每个节点分别构建自己的计算图。

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%204.png)

## 数学公式

$$
h_v^{(0)}=x_v \\ h_v^{(k+1)}=\sigma(W_k\sum_{u\in N(v)}\frac{h_u^{(k)}}{|N(v)|})\\ z_v=h_v^K
$$

1. $v$节点对应的初始隐藏层就是该节点对应的向量embedding。
2. 后面第$k+1$层隐藏层等于上一层所有节点的邻居的隐藏层的隐藏状态求和再取平均后乘以权重$W_k$，再平滑化。
3. 输出$z_v$就是第$K$（网络的深度）层的隐藏状态，即$h_v^K$。

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%205.png)

- 矩阵表达：
    - 令$H^{(k)}=[h_1^{(k)}~\dots~ h_{|V|}^{(k)}]^T$ ，有$\sum_{u\in N_v}h_u^{(k)}=A_vH^{(k)}$，其中$A_v$是图的邻接矩阵的第$v$行。
    - 令对角矩阵$D_{v,v}=Deg(v)=|N(v)|$，有$\sum_{u\in N(v)}\frac{h_u^{(k-1)}}{|N(v)|}=D^{-1}AH^{(k)}$

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%206.png)

- 这张图展示了通过对$H^{(k)}$左乘$A_v$能够取得特定的一行。
- 上面算法的一个问题在于节点更新时只考虑了自己的度，而并没有考虑到其他节点的度！（下一层每个节点的隐藏状态通过对上一层邻居节点的隐藏状态简单平均而来，没有权重）
- 一个简单的解决方案：$A_{naive}=D^{-1}AD^{-1}$。问题在于这样得到的矩阵会让前面的向量“变小”。
- 最后的normalize方案：$A_{sym}=D^{-1/2}AD^{-1/2}$。也就是更新公式最后写为
    
    $$
    D^{-1/2}AD^{-1/2}H^{(k)}\\ H^{(l+1)}=\sigma(\tilde D^{-1/2}\tilde A\tilde D^{-1/2}H^{(l)}W^{(l)})
    $$
    

## 计算图的进一步优化

- 希望节点$v$的下一层隐藏状态也由自己（$v$）决定。

![从右边可以看出每个节点不仅由邻居决定](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%207.png)

从右边可以看出每个节点不仅由邻居决定

$$
H_i^k=\sigma(\sum_{j\in N(i)}\frac{A_{ij}}{\sqrt{D_{ii}D_{jj}}}H_j^{k-1}W^k+\frac{1}{D_i}H_i^{k-1}W^k)\\ =\sigma(\sum_{j\in N(i)\cup i}\frac{A_{ij}}{\sqrt{D_{ii}D_{jj}}}H_j^{k-1}W^k)
$$

- 还有一种思路，对邻居节点和自己的隐藏层分别用不同的权重：

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%208.png)

$$
H^{(k+1)}=\sigma(\tilde AH^{(k)}W_k^T+H^{(k)}B_k^T),\text{ where }\tilde A=D^{-1/2}AD^{-1/2}
$$

## 图神经网络的训练（待补充）

# 关系图卷积神经网络

## 背景

前面讨论到的许多图神经网络的一个核心假设是：图中的节点都是“同质”的，即所有节点都代表同样的数据类型，比如一个分子中的原子、人际网中的个体等等。

但是在现实问题中，很多时候一个图是“异质”的。这也就是说，不同的节点可能是不同的“数据”或“角色”，而不同的边也可能有不同的含义。

![异质图（中国计算机学会）](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%209.png)

异质图（中国计算机学会）

一个异质图一般可以被这样定义：$G=(V,E,R,T)$，其中$V$是节点，$R$是关系，$(v_i,r,v_j)\in E$刻画两个节点之间的关系，$T(v_i)$表示节点的类型。

## 关系图卷积神经网络

### 从GCN的递推关系入手：

我们还记得，在图卷积神经网络GCN中，下一层的隐藏状态可以这样计算：

$$
h_i^{(l+1)}=\sigma(\sum_{j\in N(i)}1/c_i W^{(l)}h_j^{(l)}+W_0^{(l)}h_i^{(l)})
$$

一个非常自然的想法是，对每个关系类型$r$学习一个权重$W_r^{(l)}$，如下图所示

![Untitled](%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%208c0ea6e82a96463686f65be28affc4f7/Untitled%2010.png)

这样可以得到R-GCN的层间递推关系：

$$
h_i^{(l+1)}=\sigma(\sum_{r\in R}\sum_{j\in N_i^r}1/c_{i,r}W_r^{(l)}h_j^{（l）}+W_0^{(l)}h_i^{(l)})
$$

能够形象看出，其实相当于对每种“关系”都单独使用GCN然后求和，传向下一层。

### 可学习参数的正则化（待补充）

# 图注意力网络

[1-图注意力机制的作用与方法_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Yt421V7Bd?p=24&vd_source=f3c6b23f75d311364ca35ca0efb06c7f)

## 什么是attention?

所谓的attention，在深度学习中能够使得网络在处理时序信息时能够做到：

1. 选择性的特别关注一些信息
2. 忽略价值不大的信息

注意力机制一般通过矩阵乘法的方式来实现。

[Transformer](%E8%AF%BB%E8%AE%BA%E6%96%87%20d9f7a598fba74db9b21452e4f131ce80/Transformer%20390021fbbe9c4423a0055109f3400442.md)